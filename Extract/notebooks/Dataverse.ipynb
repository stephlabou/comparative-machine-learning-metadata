{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbdd694a",
   "metadata": {},
   "source": [
    "# Harvard Dataverse API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659bb714",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c61718d",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ac624a",
   "metadata": {},
   "source": [
    "This notebook utilizes the Harvard Dataverse API. Follow these steps in order to get the necessary credentials to continue:\n",
    "\n",
    "1. Create a Harvard Dataverse account at [Harvard Dataverse](https://dataverse.harvard.edu/dataverseuser.xhtml;jsessionid=797ccf2a28f987da3f1895ad81df?editMode=CREATE&redirectPage=%2Fdataverse_homepage.xhtml)\n",
    "2. After logging in, click on the user dropdown menu in the top right corner, and click on 'API Token'\n",
    "3. Click on 'Create Token' to receive API Token\n",
    "4. Load API Token:\n",
    "    - For repeated use, follow the ```pickle_tutorial.ipynb``` instructions to create create a ```./credentials.pkl``` file that holds a dictionary containing the entry ```{'DATAVERSE_TOKEN': MYKEY}```, with MYKEY being your API key.\n",
    "    - For sparser use, users can run the credentials cell and paste their API key when prompted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26a5022",
   "metadata": {},
   "source": [
    "## Additional Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e4d29b",
   "metadata": {},
   "source": [
    "Documentation Guide:\n",
    "- Dataverse API ([Dataverse](https://guides.dataverse.org/en/latest/user/index.html))\n",
    "- Harvard Dataverse ([Harvard](https://dataverse.harvard.edu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bca589",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4315e3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # For querying data from API\n",
    "import pandas as pd # For storing/manipulating query data\n",
    "from tqdm import tqdm # Gives status bar on loop completion\n",
    "import itertools # For efficient looping over queries\n",
    "from collections import OrderedDict\n",
    "from flatten_json import flatten\n",
    "import re\n",
    "\n",
    "import pickle # For loading credentials\n",
    "\n",
    "# Scraping imports (for metadata)\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support.select import By\n",
    "import selenium.webdriver.support.expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from bs4 import BeautifulSoup # Parsing scraped webpage data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eab76fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials\n",
    "try:\n",
    "    with open('credentials.pkl', 'rb') as credentials:\n",
    "        DATAVERSE_TOKEN = pickle.load(credentials)['DATAVERSE_TOKEN']\n",
    "except:\n",
    "    DATAVERSE_TOKEN = input('Please enter your Dataverse API Key: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d6318b",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d1a5a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba950cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'https://dataverse.harvard.edu/api'\n",
    "file_url = 'https://dataverse.harvard.edu/file.xhtml?fileId='\n",
    "HEADERS = {'X-Dataverse-key': DATAVERSE_TOKEN}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcf4d7b",
   "metadata": {},
   "source": [
    "## Extracting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3ac626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_search_outputs(search_terms, search_types, flatten_output=False):\n",
    "    \"\"\"Call the Dataverse API for each search term. \n",
    "    \n",
    "    Results are retured in results[(search_term)] = df\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    search_terms : list-like\n",
    "        collection of search terms to query over.\n",
    "    search_types : list-like\n",
    "        collection of objects to search over (must be either dataset or file).\n",
    "    flatten_output : boolean, optional (default=False)\n",
    "        Flag for specifying if nested output should be flattened.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    results : dict\n",
    "        dictionary consisting of returned DataFrames from get_search_output for each query.\n",
    "    \"\"\"\n",
    "\n",
    "    results = OrderedDict()\n",
    "\n",
    "    for search_term, search_type in tqdm(itertools.product(search_terms, search_types)):\n",
    "        results[(search_term, search_type)] = get_individual_search_output(search_term, search_type, flatten_output)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092c8c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_major_minor_version(row):\n",
    "    major = int(row['majorVersion'])\n",
    "    minor = int(row['minorVersion'])\n",
    "    return float(f'{major}.{minor}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd5490f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_individual_search_output(search_term, search_type, flatten_output=False):\n",
    "    \"\"\"Calls the Dataverse API with the specified search term and returns the search output results.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    search_term : str\n",
    "    search_type : str\n",
    "    flatten_output : boolean, optional (default=False)\n",
    "        Flag for specifying if nested output should be flattened.\n",
    "   \n",
    "    Returns\n",
    "    -------\n",
    "    search_df : DataFrame\n",
    "        DataFrame containing the output of the search query.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set search URL\n",
    "    search_url = f'{BASE_URL}/search'\n",
    "    \n",
    "    # Make sure out input is valid\n",
    "    assert isinstance(search_term, str), 'Search term must be a string'\n",
    "    assert search_type in ('dataset', 'file'), 'Search can only be conducted over \"dataset\" or \"file\"'\n",
    "    \n",
    "    # Set search parameters\n",
    "    start = 0\n",
    "    page_size = 100\n",
    "    search_df = pd.DataFrame()\n",
    "    \n",
    "    search_params = {\n",
    "        'q': search_term,\n",
    "        'per_page': page_size,\n",
    "        'start': start,\n",
    "        'type': search_type\n",
    "    }\n",
    "    \n",
    "    # Conduct initial query, extract json results\n",
    "    response = requests.get(search_url, params=search_params, headers=HEADERS)\n",
    "    output = response.json()\n",
    "    output = output['data']\n",
    "    \n",
    "    # Search until no more items are returned\n",
    "    while output.get('items'):\n",
    "        # Extract relevant output data\n",
    "        output = output['items']\n",
    "        \n",
    "        # Flatten output if necessary\n",
    "        if flatten_output:\n",
    "            output = [flatten(result) for result in output]\n",
    "        \n",
    "        output_df = pd.DataFrame(output)\n",
    "        output_df['page'] = search_params['start'] // search_params['per_page'] + 1\n",
    "        \n",
    "        search_df = pd.concat([search_df, output_df]).reset_index(drop=True)\n",
    "        \n",
    "        # Increment result offset to perform another search\n",
    "        search_params['start'] += search_params['per_page']\n",
    "        \n",
    "        # Perform next search and convert results to json\n",
    "        response = requests.get(search_url, params=search_params, headers=HEADERS)\n",
    "        output = response.json()\n",
    "        output = output['data']\n",
    "    \n",
    "    if 'majorVersion' in search_df.columns and 'minorVersion' in search_df.columns:\n",
    "        # Drop null versions since version is required for metadata extraction\n",
    "        search_df = search_df.dropna(subset = ('majorVersion', 'minorVersion'), how='any')\n",
    "        # Add query-friendly dataset version column (for metadata extraction)\n",
    "        search_df['version'] = search_df.apply(_convert_major_minor_version, axis=1)\n",
    "    \n",
    "    if search_type == 'file':\n",
    "        search_df['url'] = search_df.apply(lambda x: f'{file_url}{x.file_id}', axis=1)\n",
    "\n",
    "    return search_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345cc584",
   "metadata": {},
   "source": [
    "### Run initial API query functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aad872",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = ['\"artificial intelligence\"']\n",
    "search_types = ['dataset', 'file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec180f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "search_output_dict = get_all_search_outputs(search_terms, search_types, flatten_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f5298c",
   "metadata": {},
   "source": [
    "#### Take a look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e54f9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_key = (search_terms[0], search_types[0])\n",
    "sample_df = search_output_dict[sample_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617f442f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bd1d0c",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1b7755",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_metadata_attribute_paths = {\n",
    "    'deposit_date': '#metadata_dateOfDeposit > td',\n",
    "    'num_downloads': '#metrics-body > div'\n",
    "}\n",
    "\n",
    "file_metadata_attribute_paths = {\n",
    "    'deposit_date': '#fileDepositDateBlock > td',\n",
    "    'num_downloads': '#metrics-body > div'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f583c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4577e917",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478e0e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_results(results):\n",
    "    \"\"\"Cleans the results scraped from the page.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    results : dict\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    results : dict\n",
    "    \"\"\"\n",
    "    \n",
    "    num_downloads = results.get('num_downloads')\n",
    "\n",
    "    if num_downloads:\n",
    "        results['num_downloads'] = re.findall('\\d+', num_downloads)[0]\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e3398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attribute_value(soup, path):\n",
    "    try:\n",
    "        return soup.select_one(path).text\n",
    "    except AttributeError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7125fc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attribute_values(driver, **kwargs):\n",
    "    \"\"\"Returns attribute values for all relevant given attribute path dicts.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    driver : WebDriver\n",
    "        Selenium webdriver to use for html extraction.\n",
    "    kwargs : dict, optional\n",
    "        Attribute dicts to parse through. Accepts landing page, metadata, and terms dicts.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    attribute_value_dict : dict\n",
    "    \"\"\"\n",
    "    \n",
    "    attribute_value_dict = dict()\n",
    "    \n",
    "    # Extract attribute path dicts\n",
    "    landing_attribute_paths = kwargs.get('landing_attribute_paths')\n",
    "    metadata_attribute_paths = kwargs.get('metadata_attribute_paths')\n",
    "    terms_attribute_paths = kwargs.get('terms_attribute_paths')\n",
    "    \n",
    "    if landing_attribute_paths:\n",
    "        # Retrieve html data and create parsable object\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html)\n",
    "        \n",
    "        landing_attribute_values = {attribute: get_attribute_value(soup, path) \n",
    "                                    for attribute, path in landing_attribute_paths.items()}\n",
    "        attribute_value_dict = {**attribute_value_dict, **landing_attribute_values}\n",
    "    if metadata_attribute_paths:\n",
    "        driver.find_element_by_link_text('Metadata').click()\n",
    "        \n",
    "        # Retrieve html data and create parsable object\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html)\n",
    "        \n",
    "        metadata_attribute_values = {attribute: get_attribute_value(soup, path) \n",
    "                                    for attribute, path in metadata_attribute_paths.items()}\n",
    "        attribute_value_dict = {**attribute_value_dict, **metadata_attribute_values}\n",
    "    if terms_attribute_paths:\n",
    "        driver.find_element_by_link_text('Terms').click()\n",
    "        \n",
    "        # Retrieve html data and create parsable object\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html)\n",
    "        \n",
    "        terms_attribute_values = {attribute: get_attribute_value(soup, path) \n",
    "                                    for attribute, path in terms_attribute_paths.items()}\n",
    "        attribute_value_dict = {**attribute_value_dict, **terms_attribute_values}\n",
    "        \n",
    "    return attribute_value_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8413b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_metadata(object_paths, driver, flatten_output=False, **kwargs):\n",
    "    \"\"\"Retrieves the dataset metadata for the object/objects listed in object_paths\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    object_paths : str/list-like\n",
    "    flatten_output : boolean, optional (default=False)\n",
    "        Flag for specifying if nested output should be flattened.\n",
    "    kwargs : dict, optional\n",
    "        Additional parameters, including attribute path dictionaries.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    metadata_df : DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    metadata_df = pd.DataFrame()\n",
    "    \n",
    "    for object_path in tqdm(object_paths):\n",
    "        object_dict = dict()\n",
    "        \n",
    "        # Retrieve webpage\n",
    "        driver.get(object_path)\n",
    "        \n",
    "        # Extract & clean attribute values\n",
    "        object_dict = get_attribute_values(driver, **kwargs)\n",
    "        object_dict['url'] = object_path\n",
    "        object_dict = clean_results(object_dict)\n",
    "        \n",
    "        # Add results to DataFrame\n",
    "        metadata_df = metadata_df.append(object_dict, ignore_index=True)\n",
    "    \n",
    "    return metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5641e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_metadata(search_output_dict, flatten_output=False):\n",
    "    \"\"\"Retrieves all of the metadata that relates to the provided DataFrames\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    search_output_dict : dict\n",
    "        Dictionary of DataFrames from get_all_search_outputs.\n",
    "    flatten_output : bool, optional (default=False)\n",
    "        Flag for flattening nested columns of output.\n",
    "      \n",
    "    Returns\n",
    "    -------\n",
    "    metadata_dict : OrderedDict\n",
    "        OrderedDict of DataFrames with metadata for each query.\n",
    "        Order matches the order of search_output_dict.\n",
    "    \"\"\"\n",
    "    \n",
    "    metadata_dict = OrderedDict()\n",
    "\n",
    "    for query, df in search_output_dict.items():\n",
    "        search_term, search_type = query\n",
    "        \n",
    "        object_paths = df['url']\n",
    "        \n",
    "        metadata_dict[query] = get_query_metadata(object_paths, driver, flatten_output, **path_dict[search_type])\n",
    "    \n",
    "    return metadata_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b4c251",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dict = {\n",
    "    'dataset': {\n",
    "        'metadata_attribute_paths': dataset_metadata_attribute_paths\n",
    "    },\n",
    "    'file': {\n",
    "        'landing_attribute_paths': file_metadata_attribute_paths\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac719673",
   "metadata": {},
   "source": [
    "### Get Metadata Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff127c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dict = get_all_metadata(search_output_dict, flatten_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad592bf",
   "metadata": {},
   "source": [
    "## Combine all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc162c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_search_and_metadata_dicts(search_dict, metadata_dict, on=None, left_on=None, right_on=None, save=False):\n",
    "    \"\"\"Merges together all of the search and metadata DataFrames by the given 'on' key.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    search_dict : dict\n",
    "        Dictionary of search output results.\n",
    "    metadata_dict : dict\n",
    "        Dictionary of metadata results.\n",
    "    on : str/list-like\n",
    "        Column name(s) to merge the two dicts on.\n",
    "    left_on : str/list-like\n",
    "        Column name(s) to merge the left dict on.\n",
    "    right_on : str/list-like\n",
    "        Column name(s) to merge the right dict on.\n",
    "    save : boolean, optional (default=False)\n",
    "        Specifies if the output DataFrames should be saved.\n",
    "        If True: saves to file of format 'data/figshare/figshare_{search_term}_{search_type}.csv'.\n",
    "        If list-like: saves to respective location in list of save locations.\n",
    "            Must contain enough strings (one per query; len(search_terms) * len(search_types)).\n",
    "            \n",
    "    If the on/left_on/right_on values are not explicitely specified, behavior defaults to what is done\n",
    "    in the pandas documentation.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df_dict : OrderedDict\n",
    "        OrderedDict containing all of the merged search/metadata dicts.\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure the dictionaries contain the same searches\n",
    "    assert search_dict.keys() == metadata_dict.keys(), 'Dictionaries must contain the same searches'\n",
    "    \n",
    "    num_dataframes = len(search_dict)\n",
    "    \n",
    "    # Ensure the save variable data is proper\n",
    "    try:\n",
    "        if isinstance(save, bool):\n",
    "            save = [save] * num_dataframes\n",
    "        assert len(save) == num_dataframes\n",
    "    except:\n",
    "        raise ValueError('Incorrect save value(s)')\n",
    "        \n",
    "    # Merge the DataFrames\n",
    "    df_dict = OrderedDict()\n",
    "    for (query_key, search_df), (query_key, metadata_df), save_loc in zip(search_dict.items(), \n",
    "                                                                          metadata_dict.items(), \n",
    "                                                                          save):\n",
    "\n",
    "        # Merge small version of \"full\" dataframe with \"detailed\" dataframe\n",
    "        df_all = pd.merge(search_df, metadata_df, on=on, left_on=left_on, right_on=right_on, how='outer')\n",
    "            \n",
    "        # Save DataFrame\n",
    "        if save_loc:\n",
    "            data_dir = os.path.join('data', 'dataverse')\n",
    "            if isinstance(save_loc, str):\n",
    "                output_file = save_loc\n",
    "            elif isinstance(save_loc, bool):\n",
    "                # Ensure figshare directory is already created\n",
    "                if not os.path.isdir(data_dir):\n",
    "                    os.path.mkdir(data_dir)\n",
    "                \n",
    "                search_term, search_type = query_key\n",
    "                output_file = f'{search_term}_{search_type}.csv'\n",
    "            else:\n",
    "                raise ValueError(f'Save type must be bool or str, not {type(save_loc)}')\n",
    "\n",
    "            search_df.to_csv(os.path.join(data_dir, output_file), index=False)\n",
    "        \n",
    "        df_dict[query_key] = df_all\n",
    "    \n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e76bc66",
   "metadata": {},
   "source": [
    "### Run merge function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708c9ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = merge_search_and_metadata_dicts(search_output_dict, metadata_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e635ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict[sample_key]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
